{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DOJO-Smart-Ways/DOJO-Beam-Transforms/blob/pbi-footprint/pbi_footprint/pbi_pfx_nsap_pipeline_to_trusted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MXwhtG7xUCw"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/DOJO-Smart-Ways/DOJO-Beam-Transforms.git@pbi-footprint#egg=dojo-beam-transforms\n",
        "#!pip install git+https://github.com/DOJO-Smart-Ways/DOJO-Beam-Transforms.git#egg=dojo-beam-transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPVN3uz2igZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762a5c8c-771a-4678-dc0a-c18eb5b6ce32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n"
          ]
        }
      ],
      "source": [
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "\n",
        "import os\n",
        "\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Google Auth\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')\n",
        "\n",
        "# GCP Project\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"]= 'nidec-ga'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBhaeHle3Kfw"
      },
      "outputs": [],
      "source": [
        "#from pipeline_components.input_file import read_excels_union\n",
        "from pipeline_components import data_enrichment as de\n",
        "from pipeline_components import data_cleaning as dc\n",
        "from pipeline_components import data_understand as du"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_options = {\n",
        "    'project':'nidec-ga',\n",
        "    'runner':'DataflowRunner',\n",
        "    'region':'us-central1',\n",
        "    'staging_location':'gs://nidec-ga-temp/data-flow-pipelines/pbi-footprint/staging',\n",
        "    'temp_location':'gs://nidec-ga-temp/data-flow-pipelines/pbi-footprint/temp',\n",
        "    'template_location':'gs://nidec-ga-temp/data-flow-pipelines/pbi-footprint/template/pbi-pfx-nsap',\n",
        "    'sdk_container_image': 'us-central1-docker.pkg.dev/nidec-ga/dojo-beam/dojo_beam',\n",
        "    'sdk_location': 'container'\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "pipeline_options = {\n",
        "    'project':'nidec-ga'\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "pipeline_options = PipelineOptions.from_dictionary(pipeline_options)\n",
        "pipeline = beam.Pipeline(options=pipeline_options)"
      ],
      "metadata": {
        "id": "ZVemPNwDQY1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aux Classes"
      ],
      "metadata": {
        "id": "B2eL4a2NYl2U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcwYbjAvf4ii"
      },
      "outputs": [],
      "source": [
        "class DeduplicateByColumnFn(beam.DoFn):\n",
        "    def __init__(self, columns):\n",
        "        self.seen = set()\n",
        "        self.columns = columns\n",
        "\n",
        "    def process(self, element):\n",
        "        # Verifica se 'element' é um dicionário\n",
        "        if isinstance(element, dict):\n",
        "            # Cria uma tupla de valores das colunas especificadas\n",
        "            values = tuple(element.get(col, None) for col in self.columns)\n",
        "        else:\n",
        "            # Se não for um dicionário, considera o próprio elemento como chave\n",
        "            values = (element,)\n",
        "\n",
        "        # Verifica se já vimos a combinação de valores das colunas antes\n",
        "        if values not in self.seen:\n",
        "            # Se não, adiciona à lista de observados e produz o elemento\n",
        "            self.seen.add(values)\n",
        "            yield element\n",
        "\n",
        "class TrimValues(beam.DoFn):\n",
        "    def __init__(self, columns):\n",
        "        \"\"\"\n",
        "        Initializes the TrimValues instance.\n",
        "\n",
        "        Args:\n",
        "            columns (list of str): A list of column names to trim spaces from.\n",
        "        \"\"\"\n",
        "        self.columns = columns\n",
        "\n",
        "    def process(self, element):\n",
        "        \"\"\"\n",
        "        Processes each element, trimming leading and trailing spaces from specified columns.\n",
        "\n",
        "        Args:\n",
        "            element (dict): The input element to process, where keys are column names.\n",
        "        \"\"\"\n",
        "        for column in self.columns:\n",
        "            if column in element and isinstance(element[column], str):\n",
        "                element[column] = element[column].strip()\n",
        "        yield element\n",
        "\n",
        "\n",
        "class NormalDistributedSampler(beam.DoFn):\n",
        "    \"\"\"A DoFn implementation that samples rows based on normally distributed random numbers.\"\"\"\n",
        "\n",
        "    def __init__(self, mean, std_dev, threshold):\n",
        "        \"\"\"Initializer.\n",
        "\n",
        "        Args:\n",
        "            mean (float): The mean of the normal distribution.\n",
        "            std_dev (float): The standard deviation of the normal distribution.\n",
        "            threshold (float): A threshold to decide whether to keep a row. Rows with a generated value less or equal to this threshold will be kept.\n",
        "        \"\"\"\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_dev\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def process(self, element):\n",
        "        import numpy as np\n",
        "        \"\"\"Processes each element, deciding whether to keep it based on a normally distributed random number.\n",
        "\n",
        "        Args:\n",
        "            element: The input element to process.\n",
        "\n",
        "        Yields:\n",
        "            The input element if a generated normally distributed random number is less or equal to the threshold.\n",
        "        \"\"\"\n",
        "        # Generate a normally distributed random number\n",
        "        random_value = np.random.normal(self.mean, self.std_dev)\n",
        "\n",
        "        # Keep the element based on the threshold condition\n",
        "        if random_value <= self.threshold:\n",
        "            yield element\n",
        "\n",
        "\n",
        "class ProbabilisticSampler(beam.DoFn):\n",
        "    \"\"\"A DoFn implementation that probabilistically samples rows.\"\"\"\n",
        "\n",
        "    def __init__(self, probability):\n",
        "        \"\"\"Initializer.\n",
        "\n",
        "        Args:\n",
        "            probability (float): The probability of keeping a row. Should be between 0 and 1.\n",
        "        \"\"\"\n",
        "        self.probability = probability\n",
        "\n",
        "    def process(self, element):\n",
        "        import random\n",
        "        \"\"\"Processes each element, randomly deciding whether to keep it.\n",
        "\n",
        "        Args:\n",
        "            element: The input element to process.\n",
        "\n",
        "        Yields:\n",
        "            The input element, with a probability of `self.probability`.\n",
        "        \"\"\"\n",
        "        if random.random() < self.probability:\n",
        "            yield element\n",
        "\n",
        "\n",
        "\n",
        "class GetFirstSCharactersForColumnFn(beam.DoFn):\n",
        "    def __init__(self, column_name):\n",
        "        self.column_name = column_name\n",
        "\n",
        "    def process(self, element):\n",
        "        # Check if the specified column exists in the element (which is a dictionary)\n",
        "        if self.column_name in element:\n",
        "            # Extract the value for the specified column and truncate to first 6 characters\n",
        "            truncated_value = element[self.column_name][:10]\n",
        "            # Update the element with the truncated value\n",
        "            element[self.column_name] = truncated_value\n",
        "        # Yield the modified element\n",
        "        yield element\n",
        "\n",
        "\n",
        "class ExtractAndCheckDistinctValuesFn(beam.DoFn):\n",
        "    def __init__(self, column_name):\n",
        "        self.column_name = column_name\n",
        "\n",
        "    def process(self, element):\n",
        "        # Check if the column_name key exists in the element\n",
        "        if self.column_name in element:\n",
        "            yield element[self.column_name]\n",
        "        else:\n",
        "            # Option 1: Yield a special value (e.g., None) if the key doesn't exist\n",
        "            yield None\n",
        "\n",
        "            # Option 2: Do nothing (skip the element) if the key doesn't exist\n",
        "            #pass\n",
        "\n",
        "\n",
        "class EnsureSchemaOrderDoFn(beam.DoFn):\n",
        "    def __init__(self, schema_str):\n",
        "        \"\"\"\n",
        "        Initializes the DoFn with a schema string.\n",
        "\n",
        "        Args:\n",
        "            schema_str (str): A multiline string representing the schema.\n",
        "        \"\"\"\n",
        "        # Parse the schema string to extract field names\n",
        "        self.schema_fields = [line.split(\":\")[0].strip() for line in schema_str.strip().split(\"\\n\")]\n",
        "\n",
        "    def process(self, element):\n",
        "        \"\"\"\n",
        "        Orders the element according to the schema, maintaining a dictionary format.\n",
        "\n",
        "        Args:\n",
        "            element: A dictionary representing a single record.\n",
        "\n",
        "        Yields:\n",
        "            A dictionary with keys ordered according to the schema.\n",
        "        \"\"\"\n",
        "        ordered_dict = {field: element.get(field, None) for field in self.schema_fields}\n",
        "        yield ordered_dict\n",
        "\n",
        "\n",
        "\n",
        "class DateTimeToString(beam.DoFn):\n",
        "    def __init__(self, column_name):\n",
        "        # Initialize with the name of the column to update\n",
        "        self.column_name = column_name\n",
        "\n",
        "    def process(self, element):\n",
        "        datetime_value = element.get(self.column_name,'')\n",
        "        # Convert the datetime value to string in the specified format\n",
        "        datetime_string = datetime_value.strftime('%Y-%m-%d')\n",
        "\n",
        "        # Update the specified column with the datetime string\n",
        "        element[self.column_name] = datetime_string\n",
        "\n",
        "        # Output the modified element\n",
        "        yield element\n",
        "\n",
        "\n",
        "class GenerateStatistics(beam.PTransform):\n",
        "    def expand(self, pcollection):\n",
        "        def compute_tfdv_statistics(elements):\n",
        "            # Converter elements para um DataFrame e gerar estatísticas com TFDV\n",
        "            import pandas as pd\n",
        "            import tensorflow_data_validation as tfdv\n",
        "            dataframe = pd.DataFrame(elements)\n",
        "            stats = tfdv.generate_statistics_from_dataframe(dataframe)\n",
        "            return [stats]\n",
        "\n",
        "        return (\n",
        "            pcollection\n",
        "            | \"CollectIntoBatch\" >> beam.BatchElements()\n",
        "            | \"ComputeTFDVStatistics\" >> beam.Map(compute_tfdv_statistics)\n",
        "        )\n",
        "\n",
        "\n",
        "class ProcessExcelFiles(beam.DoFn):\n",
        "    def __init__(self, skip_rows=None, tab=None, column_date=None, parse_dates=False, converters=None):\n",
        "        self.skip_rows = skip_rows\n",
        "        self.tab = 0 if tab is None else tab\n",
        "        self.column_date = column_date\n",
        "        self.parse_dates = parse_dates\n",
        "        self.converters = converters\n",
        "\n",
        "    def process(self, file):\n",
        "        import pandas as pd\n",
        "        from apache_beam.io.filesystems import FileSystems\n",
        "        from datetime import datetime\n",
        "        import apache_beam as beam\n",
        "\n",
        "        try:\n",
        "            with FileSystems.open(file.metadata.path) as f:\n",
        "                df = pd.read_excel(f, skiprows=self.skip_rows, sheet_name=self.tab, parse_dates=self.parse_dates, converters=self.converters)\n",
        "                for col in self.column_date if self.column_date is not None else []:\n",
        "                    # Certifique-se de que a coluna existe no DataFrame para evitar KeyError.\n",
        "                    if col in df.columns:\n",
        "                        # Convertendo a coluna para datetime e então formatando a data.\n",
        "                        df[col] = pd.to_datetime(df[col]).dt.strftime('%Y-%m-%d')\n",
        "\n",
        "            df = df.dropna(how='all')\n",
        "\n",
        "            if isinstance(self.tab, int):\n",
        "                xls = pd.ExcelFile(file.metadata.path)\n",
        "                sheet_name = xls.sheet_names[self.tab]\n",
        "            else:\n",
        "                sheet_name = self.tab\n",
        "\n",
        "            df['SOURCE'] = f\"{file.metadata.path}/{sheet_name}\"\n",
        "\n",
        "            yield from df.to_dict('records')\n",
        "\n",
        "        except Exception as e:\n",
        "            yield beam.pvalue.TaggedOutput('error', {'path': file.metadata.path, 'error': str(e)})\n",
        "\n",
        "\n",
        "def read_excels_union(pipeline, input_pattern, identifier='', skiprows=None, tab=None, column_date=None, parse_dates=False, converters=None):\n",
        "    read_files = (\n",
        "        pipeline\n",
        "        | f'Match Files {identifier}' >> beam.io.fileio.MatchFiles(input_pattern)\n",
        "        | f'Read Matches {identifier}' >> beam.io.fileio.ReadMatches()\n",
        "    )\n",
        "    processed_files = read_files | f'Process Excel Files {identifier}' >> beam.ParDo(ProcessExcelFiles(skip_rows=skiprows, tab=tab, column_date=column_date, parse_dates=parse_dates, converters=converters)).with_outputs('error', main='main')\n",
        "\n",
        "    main_output = processed_files.main\n",
        "    error_output = processed_files.error\n",
        "\n",
        "    return main_output, error_output\n",
        "\n",
        "\n",
        "class ChangeDateFormatError(beam.DoFn):\n",
        "    def __init__(self, date_columns, input_format='%Y%m%d', output_format='%Y-%m-%d %H:%M:%S'):\n",
        "        \"\"\"\n",
        "        Initialize the DoFn class.\n",
        "\n",
        "        Parameters:\n",
        "        - date_columns: The names of the columns containing the date strings.\n",
        "        - input_format: The strftime format string for the input date format.\n",
        "        - output_format: The strftime format string for the output date format.\n",
        "        \"\"\"\n",
        "        self.date_columns = date_columns\n",
        "        self.input_format = input_format\n",
        "        self.output_format = output_format\n",
        "\n",
        "    def process(self, element):\n",
        "        from datetime import datetime\n",
        "        import pandas as pd\n",
        "        import apache_beam as beam\n",
        "        \"\"\"\n",
        "        Process each element to format the dates in the specified columns.\n",
        "\n",
        "        Parameters:\n",
        "        - element: The input element to process.\n",
        "        \"\"\"\n",
        "        for date_column in self.date_columns:\n",
        "            date_str = element.get(date_column)\n",
        "            if date_str:\n",
        "                try:\n",
        "                    # Attempt to parse the date using pandas to_datetime for flexibility\n",
        "                    parsed_date = pd.to_datetime(date_str, format=self.input_format)\n",
        "                    # Format the date into the desired output format\n",
        "                    element[date_column] = parsed_date.strftime(self.output_format)\n",
        "                except Exception as e:\n",
        "                    yield beam.pvalue.TaggedOutput('error', {'element': element, 'error': str(e), 'column': date_column})\n",
        "                    return  # Skip further processing to avoid multiple error entries for the same element\n",
        "        yield element\n",
        "\n",
        "\n",
        "class VerifyStrictDateFormat(beam.DoFn):\n",
        "    def __init__(self, date_column, expected_format='%Y-%m-%d'):\n",
        "        \"\"\"\n",
        "        Inicializa a classe DoFn.\n",
        "\n",
        "        Parâmetros:\n",
        "        - date_column: O nome da coluna que contém as strings de data.\n",
        "        - expected_format: A string de formato strftime para o formato de data esperado.\n",
        "        \"\"\"\n",
        "        self.date_column = date_column\n",
        "        self.expected_format = expected_format\n",
        "\n",
        "    def process(self, element):\n",
        "        from datetime import datetime\n",
        "        import pandas as pd\n",
        "        import apache_beam as beam\n",
        "        \"\"\"\n",
        "        Processa cada elemento para verificar se as datas na coluna especificada\n",
        "        estão no formato esperado.\n",
        "\n",
        "        Parâmetros:\n",
        "        - element: O elemento de entrada para processar.\n",
        "        \"\"\"\n",
        "        date_value = element.get(self.date_column)\n",
        "\n",
        "\n",
        "        try:\n",
        "            # Convert to datetime and back to string if necessary\n",
        "            if isinstance(date_value, datetime):\n",
        "                element[self.date_column] = date_value.strftime(self.expected_format)\n",
        "            elif isinstance(date_value, str):\n",
        "                # Using pd.to_datetime for string conversion\n",
        "                datetime_obj = pd.to_datetime(date_value, format=self.expected_format, errors='coerce')\n",
        "                if pd.isnull(datetime_obj):\n",
        "                    raise ValueError(f\"Invalid date format for '{date_value}'\")\n",
        "                element[self.date_column] = datetime_obj.strftime(self.expected_format)\n",
        "            else:\n",
        "                # Outputting elements with invalid date types to a separate PCollection\n",
        "                yield beam.pvalue.TaggedOutput('error', {'element':element, 'error':f\"Invalid type: {type(date_value)} for date in column '{self.date_column}'\"})\n",
        "                return\n",
        "\n",
        "            yield element\n",
        "        except Exception as e:\n",
        "            # Handling any conversion errors\n",
        "            yield beam.pvalue.TaggedOutput('error', {'element': element, 'error': str(e)})\n",
        "            return\n",
        "\n",
        "def log_error(element):\n",
        "    # Supondo que 'element' contém a informação do erro que você quer logar\n",
        "    import logging\n",
        "    logging.error(f\"Error: {element}\")\n",
        "\n",
        "\n",
        "class MaxDate(beam.DoFn):\n",
        "    def __init__(self, keys, date_column):\n",
        "        self.keys = keys  # Lista de chaves para agrupamento\n",
        "        self.date_column = date_column  # Nome da coluna de data\n",
        "\n",
        "    def process(self, element):\n",
        "        # Criar uma chave de agrupamento com os valores das chaves especificadas\n",
        "        key = tuple(element[k] for k in self.keys)\n",
        "        # Extrair a data e o elemento completo\n",
        "        date = element[self.date_column]\n",
        "        yield (key, (date, element))\n",
        "\n",
        "\n",
        "class FilterMaxDate(beam.DoFn):\n",
        "    def process(self, element):\n",
        "        (key), values = element\n",
        "        max_date, max_element = max(values)\n",
        "        max_element['maxdate_LASTUPDATEON'] = max_date\n",
        "        yield max_element\n",
        "\n",
        "\n",
        "class ExtractDateYYYYMMDD(beam.DoFn):\n",
        "    def __init__(self, column_name):\n",
        "        self.column_name = column_name\n",
        "\n",
        "    \"\"\"\n",
        "    Uma classe DoFn para extrair datas em formato YYYYMMDD de strings e\n",
        "    converter para objetos datetime.\n",
        "    \"\"\"\n",
        "    def process(self, element):\n",
        "        import re\n",
        "        # Regex para identificar uma data no formato YYYYMMDD\n",
        "        date_pattern = r'(\\d{4})(\\d{2})(\\d{2})'\n",
        "\n",
        "        # Buscar pela data no texto da coluna especificada\n",
        "        text = element.get(self.column_name, '')\n",
        "        match = re.search(date_pattern, text)\n",
        "\n",
        "        if match:\n",
        "            # Construir a data no formato YYYY-MM-DD\n",
        "            extracted_date = f'{match.group(1)}-{match.group(2)}-{match.group(3)}'\n",
        "            # Adicionar a data extraída ao elemento com uma nova chave\n",
        "            element['EXTRACT_DATE'] = extracted_date\n",
        "            yield element\n",
        "        else:\n",
        "            # Opcional: lidar com casos onde a data não é encontrada\n",
        "            # Por exemplo, pode-se escolher emitir o elemento sem modificação ou adicionar um valor padrão\n",
        "            element['EXTRACT_DATE'] = '1970-01-01'\n",
        "            yield element\n",
        "\n",
        "\n",
        "class CheckMissingValuesDoFn(beam.DoFn):\n",
        "    def __init__(self, column_list):\n",
        "        self.column_list = column_list\n",
        "\n",
        "    def process(self, element):\n",
        "        import pandas as pd\n",
        "        import apache_beam as beam\n",
        "        # Check for missing values in the specified columns\n",
        "        missing_columns = [col for col in self.column_list if pd.isnull(element.get(col))]\n",
        "        if missing_columns:\n",
        "            # If missing values are found, output to a separate PCollection for errors\n",
        "            error_msg = f\"Missing values in columns: {', '.join(missing_columns)}\"\n",
        "            yield beam.pvalue.TaggedOutput('missing_values_error', {'element': element, 'error': error_msg})\n",
        "        else:\n",
        "            yield element"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Path"
      ],
      "metadata": {
        "id": "-kPjVMvYYsDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_path(*args: str) -> str:\n",
        "    if args:\n",
        "        return \"gs://\" + \"/\".join(args)\n",
        "    else:\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "iTqyoJZq8rbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cimd_in_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\", \"*CIMD IN Spend Data*.xlsx\")\n",
        "\n",
        "def get_cimd_folder_in_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"CIMD IN\", \"*CIMD IN Spend Data*.xlsx\")\n",
        "\n",
        "def get_wet_mx_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"*WET MX Spend Data*.xlsx\")\n",
        "\n",
        "def get_wet_folder_mx_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"WET NAR\",\"*WET MX Spend Data*.xlsx\")\n",
        "\n",
        "def get_hvac_mx_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"*HVAC MX Spend Data*.xlsx\")\n",
        "\n",
        "def get_hvac_folder_mx_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"HVAC MX\",\"*HVAC MX Spend Data*.xlsx\")\n",
        "\n",
        "def get_cimd_nar_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"*CIMD NAR Spend Data*.xlsx\")\n",
        "\n",
        "def get_cimd_folder_nar_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"CIMD NAR\",\"*CIMD NAR Spend Data*.xlsx\")\n",
        "\n",
        "def get_wet_cn_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"*WET CN Spend Data*.xlsx\")\n",
        "\n",
        "def get_wet_folder_cn_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"WET CN\",\"*WET CN Spend Data*.xlsx\")\n",
        "\n",
        "def get_wet_ro_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"*WET RO Spend Data*.xlsx\")\n",
        "\n",
        "def get_wet_folder_ro_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"WET EMEA\",\"*WET RO Spend Data*.xlsx\")\n",
        "\n",
        "def get_wet_ro_intracompany_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"*WET RO Intracompany Spend Data*.xlsx\")\n",
        "\n",
        "def get_wet_mx_intracompany_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"*WET MX Intracompany Spend Data*.xlsx\")\n",
        "\n",
        "def get_wet_folder_mx_intracompany_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"WET NAR IC\",\"*WET MX Intracompany Spend Data*.xlsx\")\n",
        "\n",
        "def get_hvac_cn_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\", \"*HVAC CN Spend Data*.xlsx\")\n",
        "\n",
        "def get_hvac_folder_cn_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\", \"HVAC CN\", \"*HVAC CN Spend Data*.xlsx\")\n",
        "\n",
        "def get_cimd_folder_emea_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"CIMD EMEA\", \"*CIMD EMEA Spend Data*.xlsx\")\n",
        "\n",
        "def get_cimd_emea_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"*CIMD EMEA Spend Data*.xlsx\")\n",
        "\n",
        "def get_fir_sc_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"FIR\", \"*FIR Spend Data Subcontracting*.xlsx\")\n",
        "\n",
        "def get_fir_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"spend-data\",\"FIR\",\"*FIR Spend Data NF * *.xlsx\")\n",
        "\n",
        "def get_file_org_path() -> str:\n",
        "    return make_path(\"nidec-ga-transient\", \"pbi-supplier-footprint\", \"PFX MD.xlsx\")\n"
      ],
      "metadata": {
        "id": "qbmU7JFMQ7e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aux Lists"
      ],
      "metadata": {
        "id": "pk2A38PfY8ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rename_columns = {\n",
        "'Posting Date':'PERIOD',\n",
        "'Vendor Name':'VENDOR_NAME_LOCAL',\n",
        "'Item No.':'MATERIAL',\n",
        "'Commodity Code':'CATEGORY_LOCAL',\n",
        "'Description':'MATERIAL_DESCRIPTION',\n",
        "'Total Amount':'SPEND_PO_TC',\n",
        "'Quantity':'TRY_QTY',\n",
        "'Currency Code':'TRANSACTION_CURRENCY',\n",
        "'Payment Method Code':'PAYMENT_TERM',\n",
        "'Shipment Method Code':'INCOTERM',\n",
        "'Source No.':'VENDOR',\n",
        "'Org':'ORG',\n",
        "'Purchase Order':'PURCHASE_ORDER',\n",
        "'Item Number':'MATERIAL',\n",
        "'Trx Qty':'TRX_QTY',\n",
        "'Extended PO Price':'SPEND_PO_LC',\n",
        "'Supplier Number':'VENDOR',\n",
        "'Extended Standard Cost':'SPEND_STANDARD_LC',\n",
        "'Transaction Unit Of Measure':'UOM',\n",
        "'DRI Code':'DRI_CODE',\n",
        "'Commodity Family':'CATEGORY_LOCAL',\n",
        "'Item Description':'MATERIAL_DESCRIPTION',\n",
        "'Supplier Name':'VENDOR_NAME_LOCAL',\n",
        "'Date':'PERIOD',\n",
        "'Item':'MATERIAL',\n",
        "'Amount':'TRX_QTY',\n",
        "'Supplier Code':'VENDOR',\n",
        "'Total Cost':'SPEND_PO_TC',\n",
        "'Supplier':'VENDOR_NAME_LOCAL',\n",
        "'Division ID':'ORG',\n",
        "'PO_NUMBER':'PURCHASE_ORDER',\n",
        "'VENDOR_ID':'VENDOR',\n",
        "'ITM_GLMOCO':'MATERIAL',\n",
        "'QTY_RECVD':'TRX_QTY',\n",
        "'Currency Code':'TRANSACTION_CURRENCY',\n",
        "'Unit Price':'UNIT_COST',\n",
        "'RECV_DATE':'PERIOD',\n",
        "'VEND_NAME':'VENDOR_NAME_LOCAL',\n",
        "'COMMODITY FAMILY':'CATEGORY_LOCAL',\n",
        "'DRI CODE':'DRI_CODE',\n",
        "'Unit':'UOM',\n",
        "'Item':'MATERIAL',\n",
        "'Sup':'VENDOR',\n",
        "'Company':'ORG',\n",
        "'Gross TO':'SPEND_PO_LC',\n",
        "'Pu Qty':'TRX_QTY',\n",
        "'TransactionCurrency':'TRANSACTION_CURRENCY',\n",
        "'INT_EXT':'VENDOR_TYPE',\n",
        "'Cm_Descr':'CATEGORY_LOCAL',\n",
        "'Item Descr':'MATERIAL_DESCRIPTION',\n",
        "'Vendor ID':'VENDOR',\n",
        "'PONumber':'PURCHASE_ORDER',\n",
        "'Item Number':'MATERIAL' ,\n",
        "'Quantity':'TRX_QTY' ,\n",
        "'POReceipt Local Amount':'SPEND_PO_LC',\n",
        "'Foreign Invoice Amount':'SPEND_PO_TC',\n",
        "'Currency Code':'TRANSACTION_CURRENCY',\n",
        "'Item Description':'MATERIAL_DESCRIPTION',\n",
        "'Item Description.1':'CATEGORY_LOCAL',\n",
        "'Vendor EN_Name':'VENDOR_NAME_LOCAL',\n",
        "'Transaction Date':'PERIOD'\n",
        "}\n",
        "\n",
        "COMMON_DATE_AND_TIME_FORMATS = [\n",
        "'%Y-%m-%d',                 # ISO 8601 date format (e.g., 2024-03-28)\n",
        "'%m/%d/%Y',                 # U.S. style date (e.g., 03/28/2024)\n",
        "#'%d/%m/%Y',                 # European style date (e.g., 28/03/2024)\n",
        "'%Y-%m-%dT%H:%M:%S',        # ISO 8601 datetime format (e.g., 2024-03-28T14:00:00)\n",
        "'%Y-%m-%d %H:%M:%S',        # ISO 8601 datetime format with space (e.g., 2024-03-28 14:00:00)\n",
        "'%m/%d/%Y %H:%M:%S',        # U.S. style datetime (e.g., 03/28/2024 14:00:00)\n",
        "#'%d/%m/%Y %H:%M:%S',        # European style datetime (e.g., 28/03/2024 14:00:00)\n",
        "'%Y%m%d%H%M%S',             # Compact datetime format (e.g., 20240328140000)\n",
        "'%Y-%m-%d %H:%M:%S.%f',     # ISO 8601 with microseconds (e.g., 2024-03-28 14:00:00.123456)\n",
        "'%m/%d/%Y %I:%M:%S %p',     # U.S. style with 12-hour clock (e.g., 03/28/2024 02:00:00 PM)\n",
        "#'%d/%m/%Y %I:%M:%S %p',     # European style with 12-hour clock (e.g., 28/03/2024 02:00:00 PM)\n",
        "]\n",
        "\n",
        "schema_pbi_nsap = \"\"\"\n",
        "ORG:STRING,\n",
        "PERIOD:DATETIME,\n",
        "PURCHASE_ORDER:STRING,\n",
        "MATERIAL:STRING,\n",
        "MATERIAL_DESCRIPTION:STRING,\n",
        "UOM:STRING,\n",
        "CATEGORY_LOCAL:STRING,\n",
        "FAMILY_LOCAL:STRING,\n",
        "DRI_CODE:STRING,\n",
        "VENDOR:STRING,\n",
        "VENDOR_NAME_LOCAL:STRING,\n",
        "VENDOR_COUNTRY:STRING,\n",
        "VENDOR_TYPE:STRING,\n",
        "TRX_QTY:FLOAT,\n",
        "SPEND_PO_TC:FLOAT,\n",
        "TRANSACTION_CURRENCY:STRING,\n",
        "SPEND_PO_LC:FLOAT,\n",
        "SPEND_STANDARD_LC:FLOAT,\n",
        "LOCAL_CURRENCY:STRING,\n",
        "INCOTERM:STRING,\n",
        "PAYMENT_TERM:STRING,\n",
        "SOURCE:STRING\"\"\""
      ],
      "metadata": {
        "id": "c7dRU-evY5Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Pipeline"
      ],
      "metadata": {
        "id": "0hj1gGVkYysU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_fir() -> beam.PCollection:\n",
        "\n",
        "  input_fir_sc, error_read_fir_sc = read_excels_union(pipeline, get_fir_sc_path(), identifier='fir sc', converters={'articolo':str, 'cod_forn':str})\n",
        "  error_read_fir_sc | 'Handle Format Errors Read FIR SC' >> beam.Map(log_error)\n",
        "\n",
        "  clean_fir_sc, error_fir_sc = (input_fir_sc\n",
        "      | 'Trim columns fir sc' >> beam.ParDo(TrimValues(['deposito', 'data_reg', 'cod_forn', 'ragsoc1', 'articolo', 'descrizione', 'qta_mov_mag', 'costo_lavoro', 'numero_mov_mag', 'azienda', 'riga_mov_mag']))\n",
        "      | 'Derive Purchase Order' >> beam.ParDo(de.MergeColumnsFn([(['numero_mov_mag','riga_mov_mag'], 'PURCHASE_ORDER', '-')]))\n",
        "      | 'Keep Columns fir sc' >> beam.ParDo(dc.KeepColumns(['PURCHASE_ORDER','SOURCE','deposito', 'data_reg', 'cod_forn', 'ragsoc1', 'articolo', 'descrizione', 'qta_mov_mag', 'costo_lavoro', 'azienda']))\n",
        "      | 'Rename Columns fir sc' >> beam.ParDo(dc.RenameColumns({'deposito':'DEPOSITO', 'data_reg':'PERIOD', 'cod_forn':'VENDOR', 'ragsoc1':'VENDOR_NAME_LOCAL', 'articolo':'MATERIAL', 'descrizione':'MATERIAL_DESCRIPTION', 'qta_mov_mag':'TRX_QTY', 'costo_lavoro':'SPEND_PO_LC', 'azienda':'CENTER'}))\n",
        "      | 'Replace Values in SOURCE  fir sc' >> beam.ParDo(dc.ReplaceValues([('VENDOR', ' ', ''), ('MATERIAL', ' ', ''),  ('PURCHASE_ORDER', 'NONE', ''), ('SOURCE','gs://nidec-ga-transient/','')]))\n",
        "      | 'Convert Columns to Float fir sc' >> beam.ParDo(de.ColumnsToFloatConverter(['TRX_QTY','SPEND_PO_LC']))\n",
        "      | 'Convert Columns to String fir sc' >> beam.ParDo(de.ColumnsToStringConverter(['VENDOR', 'SOURCE', 'PURCHASE_ORDER', 'DEPOSITO', 'VENDOR_NAME_LOCAL', 'MATERIAL', 'MATERIAL_DESCRIPTION', 'CENTER']))\n",
        "      #| 'Replace Patters Material startswith 0 for \"\"  FIR SC' >> beam.ParDo(ReplacePatterns(columns=['MATERIAL'],pattern=r'^0+', replacement=''))\n",
        "      | 'Convert String Columns to fir sc' >> beam.ParDo(de.ConvertToUpperCase(['VENDOR', 'SOURCE', 'PURCHASE_ORDER', 'DEPOSITO', 'VENDOR_NAME_LOCAL', 'MATERIAL', 'MATERIAL_DESCRIPTION', 'CENTER']))\n",
        "      | 'Verify Column Date Format FIR SC' >> beam.ParDo(VerifyStrictDateFormat(date_column='PERIOD')).with_outputs('error', main='main')\n",
        "  )\n",
        "  error_fir_sc | 'Handle Format Errors FIR SC' >> beam.Map(log_error)\n",
        "\n",
        "  input_fir, error_read_fir = read_excels_union(pipeline, get_fir_path(), identifier='fir', converters={'codart':str, 'codcliforn':str})\n",
        "  input_fir_rnm = (input_fir | 'Rename Column depproprieta to match in all files' >> beam.ParDo(dc.RenameColumns({'dep_proprieta': 'depproprieta'})))\n",
        "  error_read_fir | 'Handle Format Errors Read FIR' >> beam.Map(log_error)\n",
        "\n",
        "  clean_fir, error_fir = (input_fir_rnm\n",
        "      | 'TRim columns fir' >> beam.ParDo(TrimValues(['rigadoc','deposito', 'datareg', 'numerodoc', 'codart', 'descrart', 'categ01', 'categ04', 'codcliforn', 'ragsoc', 'qta', 'importonetto', 'azienda','depproprieta','naturamov']))\n",
        "      | 'Filter rows to have only Y' >> beam.ParDo(dc.KeepColumnValues('depproprieta',['Y']))\n",
        "      | 'Filter rows to have naturamov INBOUND_MAT or PURCHASE_RETURN' >> beam.ParDo(dc.KeepColumnValues('naturamov',['INBOUND_MAT','PURCHASE_RETURN']))\n",
        "      | 'Derive Purchase Order fir' >> beam.ParDo(de.MergeColumnsFn([(['numerodoc','rigadoc'], 'PURCHASE_ORDER', '-')]))\n",
        "      | 'Keep Columns fir' >> beam.ParDo(dc.KeepColumns(['deposito', 'datareg', 'codart', 'descrart', 'categ01', 'categ04', 'codcliforn', 'ragsoc', 'qta', 'importonetto', 'azienda','SOURCE','PURCHASE_ORDER']))\n",
        "      | 'Rename Columns fir' >> beam.ParDo(dc.RenameColumns({'deposito':'DEPOSITO', 'datareg':'PERIOD', 'codart':'MATERIAL', 'descrart':'MATERIAL_DESCRIPTION', 'categ01':'FAMILY_LOCAL', 'categ04':'CATEGORY_LOCAL', 'codcliforn':'VENDOR', 'ragsoc':'VENDOR_NAME_LOCAL', 'qta':'TRX_QTY', 'importonetto':'SPEND_PO_LC', 'azienda':'CENTER'}))\n",
        "      | 'Replace Values in FIR' >> beam.ParDo(dc.ReplaceValues([('VENDOR', ' ', ''), ('MATERIAL', ' ', ''),  ('PURCHASE_ORDER', 'NONE', ''), ('SOURCE','gs://nidec-ga-transient/','')]))\n",
        "      | 'Convert Columns to Float fir' >> beam.ParDo(de.ColumnsToFloatConverter(['TRX_QTY','SPEND_PO_LC']))\n",
        "      | 'Convert Columns to String fir' >> beam.ParDo(de.ColumnsToStringConverter(['VENDOR', 'SOURCE', 'PURCHASE_ORDER', 'DEPOSITO', 'VENDOR_NAME_LOCAL', 'MATERIAL', 'MATERIAL_DESCRIPTION', 'FAMILY_LOCAL', 'CATEGORY_LOCAL', 'CENTER']))\n",
        "      | 'Convert String Columns to fir' >> beam.ParDo(de.ConvertToUpperCase(['VENDOR', 'SOURCE', 'PURCHASE_ORDER', 'DEPOSITO', 'VENDOR_NAME_LOCAL', 'MATERIAL', 'MATERIAL_DESCRIPTION', 'FAMILY_LOCAL', 'CATEGORY_LOCAL', 'CENTER']))\n",
        "      #| 'Replace Patters Material startswith 0 for \"\"  FIR' >> beam.ParDo(ReplacePatterns(columns=['MATERIAL'],pattern=r'^0+', replacement=''))\n",
        "      | 'Verify Column Date Format FIR' >> beam.ParDo(VerifyStrictDateFormat(date_column='PERIOD')).with_outputs('error', main='main')\n",
        "  )\n",
        "\n",
        "  error_fir | 'Handle Format Errors FIR' >> beam.Map(log_error)\n",
        "  union_firs = ((clean_fir,clean_fir_sc) | 'Union PCollections firs' >> beam.Flatten())\n",
        "\n",
        "  input_fir_org, error_read_fir_org = read_excels_union(pipeline, get_file_org_path(), identifier='fir org', tab='FIR_ORG')\n",
        "  error_read_fir_org | 'Handle Format Errors Read FIR ORG' >> beam.Map(log_error)\n",
        "\n",
        "  clean_fir_org = (input_fir_org\n",
        "      | 'Keep Columns fir org' >> beam.ParDo(dc.KeepColumns(['DEPOSITO', 'ORG', 'CENTER']))\n",
        "      | 'Convert String Columns to fir org' >> beam.ParDo(de.ConvertToUpperCase(['DEPOSITO', 'ORG', 'CENTER']))\n",
        "  )\n",
        "\n",
        "  group_clean_fir_org = de.key_transform(clean_fir_org, ['DEPOSITO','CENTER'], identifier='fir org grouped')\n",
        "\n",
        "  group_union_firs = de.key_transform(union_firs, ['DEPOSITO','CENTER'], identifier='union fir fir_sc grouped')\n",
        "\n",
        "  stage_fir = de.join(group_union_firs, group_clean_fir_org, method='left_join')\n",
        "\n",
        "  stage_fir = (stage_fir\n",
        "      | 'replace missvalue for fir in org' >> beam.ParDo(de.ReplaceMissingValues('ORG','FIR'))\n",
        "      | \"Ensure Schema Order FIR\" >> beam.ParDo(EnsureSchemaOrderDoFn(schema_pbi_nsap))\n",
        "  )\n",
        "\n",
        "  return stage_fir"
      ],
      "metadata": {
        "id": "RmJOo7jEOqIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cimd_in() -> beam.PCollection:\n",
        "\n",
        "  input_cimd_in, errors_read_cimd_in = read_excels_union(pipeline, get_cimd_in_path(), identifier='cimd_in', column_date=['Posting Date'], converters={'Item No.':str, 'Source No.':str})\n",
        "  input_folder_cimd_in, errors_read_folder_cimd_in = read_excels_union(pipeline, get_cimd_folder_in_path(), identifier='cimd_in_folder', column_date=['Posting Date'], converters={'Item No.':str, 'Source No.':str})\n",
        "\n",
        "  errors_read_cimd_in | 'Handle Format Errors Read CIMD IN' >> beam.Map(log_error)\n",
        "  errors_read_folder_cimd_in | 'Handle Format Errors Read Folder CIMD IN' >> beam.Map(log_error)\n",
        "\n",
        "  merged_cimd_in = ((input_folder_cimd_in,input_cimd_in) | 'Merge PCollections CIMD IN' >> beam.Flatten())\n",
        "\n",
        "  stage_cimd_in, errors_stage_cimd_in = (merged_cimd_in\n",
        "      | 'Derive PURCHASE_ORDER by MERGING CIMD IN' >> beam.ParDo(de.MergeColumnsFn([(['PO No','PO Line No'], 'PURCHASE_ORDER', '-')]))\n",
        "      | 'Replace Patters , for \"\"  CIMD' >> beam.ParDo(dc.ReplaceValues([('Total Amount', ',', ''), ('Quantity', ',', ''), ('UOM', 'NO', 'UN')]))\n",
        "      | 'Derive SPEND_PO_LC with Total Amount / Currency_Factor' >> beam.ParDo(de.GenericArithmeticOperation([{'operands':['Total Amount','Currency Factor'],'result_column':'SPEND_PO_LC','formula': lambda c1,c2: c1 / c2 if c2 else 0}]))\n",
        "      | 'Rename Columns CIMD' >> beam.ParDo(dc.RenameColumns(rename_columns))\n",
        "      | 'Derive ORG CIMD IN' >> beam.ParDo(de.GenericDeriveCondition(column=None, map=None, new_column='ORG', default='IN01'))\n",
        "      | 'Derive LOCAL_CURRENCY' >> beam.ParDo(de.GenericDeriveCondition(column=None, map=None, new_column='LOCAL_CURRENCY', default='INR'))\n",
        "      | 'Trim CIMD IN' >> beam.ParDo(TrimValues(['MATERIAL', 'VENDOR_NAME_LOCAL', 'VENDOR', 'PURCHASE_ORDER', 'MATERIAL_DESCRIPTION']))\n",
        "      | 'Replace Values in CIMD IN' >> beam.ParDo(dc.ReplaceValues([('VENDOR', ' ', ''),  ('MATERIAL', ' ', ''), ('PURCHASE_ORDER', 'NONE', ''), ('UOM', 'NO', 'UN'), ('TRANSACTION_CURRENCY', 'INR', 'LOCAL_CURRENCY'), ('TRANSACTION_CURRENCY', 'INR', 'LOCAL_CURRENCY'), ('SOURCE','gs://nidec-ga-transient/','')]))\n",
        "      | 'Convert Columns to Float CIMD' >> beam.ParDo(de.ColumnsToFloatConverter(['SPEND_PO_LC','TRX_QTY']))\n",
        "      | 'Convert Columns to String CIMD' >> beam.ParDo(de.ColumnsToStringConverter(['UOM', 'VENDOR_NAME_LOCAL', 'MATERIAL', 'CATEGORY_LOCAL', 'MATERIAL_DESCRIPTION', 'TRANSACTION_CURRENCY', 'PAYMENT_TERM', 'INCOTERM', 'PURCHASE_ORDER', 'SOURCE', 'LOCAL_CURRENCY', 'ORG', 'VENDOR']))\n",
        "      | 'Convert String Columns to UPPERCASE CIMD' >> beam.ParDo(de.ConvertToUpperCase(['UOM', 'VENDOR_NAME_LOCAL', 'MATERIAL', 'CATEGORY_LOCAL', 'MATERIAL_DESCRIPTION', 'TRANSACTION_CURRENCY', 'PAYMENT_TERM', 'INCOTERM', 'PURCHASE_ORDER', 'SOURCE', 'LOCAL_CURRENCY', 'ORG', 'VENDOR']))\n",
        "      #| 'Replace Patters Material startswith 0 for \"\"  CIMD IN' >> beam.ParDo(ReplacePatterns(columns=['MATERIAL'],pattern=r'^0+', replacement=''))\n",
        "      | \"Ensure Schema Order CIMD IN\" >> beam.ParDo(EnsureSchemaOrderDoFn(schema_pbi_nsap))\n",
        "      | 'Verify Column Date Format CIMD IN' >> beam.ParDo(VerifyStrictDateFormat(date_column='PERIOD')).with_outputs('error', main='main')\n",
        "  )\n",
        "\n",
        "  errors_stage_cimd_in | 'Handle Format Errors Cleaner CIMD IN' >> beam.Map(log_error)\n",
        "\n",
        "  return stage_cimd_in"
      ],
      "metadata": {
        "id": "XSSXP6aLP4pH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_oracle_nar() -> beam.PCollection:\n",
        "\n",
        "  input_cimd_nar, errors_read_cimd_nar = read_excels_union(pipeline, get_cimd_nar_path(), identifier='cimd_nar', column_date=['Transaction Date'], converters={'Item Number':str, 'Supplier Number':str})\n",
        "  errors_read_cimd_nar | 'Handle Format Errors Read CIMD NAR' >> beam.Map(log_error)\n",
        "\n",
        "  input_folder_cimd_nar, errors_read_folder_cimd_nar = read_excels_union(pipeline, get_cimd_folder_nar_path(), identifier='cimd_nar_folder', column_date=['Transaction Date'], converters={'Item Number':str, 'Supplier Number':str})\n",
        "  errors_read_folder_cimd_nar | 'Handle Format Errors Read Folder CIMD NAR' >> beam.Map(log_error)\n",
        "\n",
        "  input_hvac_mx, errors_read_hvac_mx = read_excels_union(pipeline, get_hvac_mx_path(), identifier='hvac_mx', column_date=['Transaction Date'], converters={'Item Number':str, 'Supplier No':str})\n",
        "  input_hvac_mx_rnm = (input_hvac_mx | 'Rename Column Vendor Not Folder' >> beam.ParDo(dc.RenameColumns({'Supplier No': 'Supplier Number'})))\n",
        "  errors_read_hvac_mx | 'Handle Format Errors Read HVAC MX' >> beam.Map(log_error)\n",
        "\n",
        "  input_folder_hvac_mx, errors_read_folder_hvac_mx = read_excels_union(pipeline, get_hvac_folder_mx_path(), identifier='hvac_mx_folder', column_date=['Transaction Date'], converters={'Item Number':str, 'Supplier No':str})\n",
        "  input_folder_hvac_mx_rnm = (input_folder_hvac_mx | 'Rename Column Vendor in Folder' >> beam.ParDo(dc.RenameColumns({'Supplier No': 'Supplier Number'})))\n",
        "  errors_read_folder_hvac_mx | 'Handle Format Errors Read Folder HVAC MX' >> beam.Map(log_error)\n",
        "\n",
        "  input_wet_mx_1, error_read_wet_mx_1 = read_excels_union(pipeline, get_wet_mx_path(), identifier='wet_mx_1', tab='A01 Paragould', column_date=['Transaction Date'], converters={'Item Number':str, 'Supplier Number':str})\n",
        "  error_read_wet_mx_1  | 'Handle Format Errors Read WET MX Sheet 1' >> beam.Map(log_error)\n",
        "\n",
        "  input_folder_wet_mx_1, error_read_folder_wet_mx_1 = read_excels_union(pipeline, get_wet_folder_mx_path(), identifier='folder_wet_mx_1', column_date=['Transaction Date'], converters={'Item Number':str, 'Supplier Number':str}, tab='A01 Paragould')\n",
        "  error_read_folder_wet_mx_1 | 'Handle Format Errors Read Folder WET MX Sheet 1' >> beam.Map(log_error)\n",
        "\n",
        "  input_wet_mx_2, error_read_wet_mx_2 = read_excels_union(pipeline, get_wet_mx_path(), identifier='wet_mx_2', column_date=['Transaction Date'], converters={'Item Number':str, 'Supplier Number':str}, tab='A02 Reynosa')\n",
        "  error_read_wet_mx_2  | 'Handle Format Errors Read WET MX' >> beam.Map(log_error)\n",
        "\n",
        "  input_folder_wet_mx_2, error_read_folder_wet_mx_2 = read_excels_union(pipeline, get_wet_folder_mx_path(), identifier='folder_wet_mx_2', column_date=['Transaction Date'], converters={'Item Number':str, 'Supplier Number':str}, tab='A02 Reynosa')\n",
        "  error_read_folder_wet_mx_2 | 'Handle Format Errors Read Folder WET MX' >> beam.Map(log_error)\n",
        "\n",
        "  merged_oracle_nar = ((\n",
        "      input_cimd_nar,\n",
        "      input_folder_cimd_nar,\n",
        "      input_hvac_mx_rnm,\n",
        "      input_folder_hvac_mx_rnm,\n",
        "      input_wet_mx_1,\n",
        "      input_folder_wet_mx_1,\n",
        "      input_wet_mx_2,\n",
        "      input_folder_wet_mx_2) | 'Merge PCollections ORACLE NAR' >> beam.Flatten())\n",
        "\n",
        "  stage_oracle_nar, error_oracle_nar = (merged_oracle_nar\n",
        "      | 'Drop Period' >> beam.ParDo(dc.DropColumns(['PERIOD']))\n",
        "      | 'Rename Columns ORACLE NAR' >> beam.ParDo(dc.RenameColumns({'Transaction Unit Of Measure':'ORG','Org':'ORG', 'Transaction Date':'PERIOD', 'Purchase Order':'PURCHASE_ORDER', 'Item Number':'MATERIAL', 'Trx Qty':'TRX_QTY', 'Extended PO Price':'SPEND_PO_LC', 'Supplier Number':'VENDOR', 'Extended Standard Cost':'SPEND_STANDARD_LC', 'Transaction Unit Of Measure':'UOM', 'DRI Code':'DRI_CODE', 'Commodity Family':'CATEGORY_LOCAL', 'Item Description':'MATERIAL_DESCRIPTION','Supplier Name':'VENDOR_NAME_LOCAL'}))\n",
        "      | 'Keep Columns ORACLE NAR' >> beam.ParDo(dc.KeepColumns(['ORG','PERIOD','PURCHASE_ORDER','MATERIAL','TRX_QTY','SPEND_PO_LC','VENDOR','SPEND_STANDARD_LC','UOM','DRI_CODE','CATEGORY_LOCAL','MATERIAL_DESCRIPTION','VENDOR_NAME_LOCAL','SOURCE']))\n",
        "      | 'Filter Col ORG with ORG ORACLE NAR' >> beam.ParDo(dc.FilterColumnValues(column='ORG', values_to_filter=['ORG']))\n",
        "      | 'Convert Columns to String ORACLE' >> beam.ParDo(de.ColumnsToStringConverter(['ORG', 'PURCHASE_ORDER', 'MATERIAL', 'VENDOR', 'SOURCE', 'VENDOR_NAME_LOCAL', 'MATERIAL_DESCRIPTION', 'UOM', 'DRI_CODE', 'CATEGORY_LOCAL']))\n",
        "      | 'Convert String Columns to UPPERCASE ORACLE' >> beam.ParDo(de.ConvertToUpperCase(['ORG', 'PURCHASE_ORDER', 'MATERIAL', 'VENDOR', 'SOURCE', 'VENDOR_NAME_LOCAL', 'MATERIAL_DESCRIPTION', 'UOM', 'DRI_CODE', 'CATEGORY_LOCAL']))\n",
        "      | 'Remove None wet mx' >> beam.ParDo(dc.ReplaceValues([('PURCHASE_ORDER', 'NONE', '')]))\n",
        "      #| 'Replace Patters Material startswith 0 for \"\"  ORACLE NAR' >> beam.ParDo(ReplacePatterns(columns=['MATERIAL'],pattern=r'^0+', replacement=''))\n",
        "      | 'Convert Columns to Float ORACLE' >> beam.ParDo(de.ColumnsToFloatConverter(['SPEND_STANDARD_LC','SPEND_PO_LC','TRX_QTY']))\n",
        "      | 'Replace Values in SOURCE ORACLE' >> beam.ParDo(\n",
        "          dc.ReplaceValues(\n",
        "              [\n",
        "                  ('SOURCE', 'GS://NIDEC-GA-TRANSIENT/', ''), ('SPEND_STANDARD_LC','$   ', ''), ('SPEND_PO_LC','$   ', ''),\n",
        "                  ('TRX_QTY',r'\\(',''), ('SPEND_PO_LC', r'\\(',''), ('SPEND_STANDARD_LC', r'\\(',''),\n",
        "                  ('TRX_QTY',r'\\)',''), ('SPEND,_PO_LC',r'\\)',''), ('SPEND_STANDARD_LC',r'\\)',''),\n",
        "                  ('TRX_QTY',',',''), ('SPEND_PO_LC', ',', ''), ('SPEND_STANDARD_LC', ',', ''),\n",
        "                  ('TRX_QTY',' ',''), ('SPEND_PO_LC', ' ', ''), ('SPEND_STANDARD_LC', ' ', ''),\n",
        "                   ('MATERIAL_DESCRIPTION', ' ', ''), ('VENDOR_NAME_LOCAL', ' ', '')\n",
        "          ])\n",
        "        )\n",
        "      | 'Clean NaN to write in BQ Oracle' >> beam.ParDo(dc.CleanNaN())\n",
        "      | \"Ensure Schema Order ORACLE NAR\" >> beam.ParDo(EnsureSchemaOrderDoFn(schema_pbi_nsap))\n",
        "      | 'Verify Column Date Format ORACLE NAR' >> beam.ParDo(VerifyStrictDateFormat(date_column='PERIOD')).with_outputs('error', main='main')\n",
        "  )\n",
        "  error_oracle_nar |  'Handle Format Errors Oracle Nar' >> beam.Map(log_error)\n",
        "\n",
        "  return stage_oracle_nar"
      ],
      "metadata": {
        "id": "qqpS3IVbP4vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_wet_mx_intracompany() -> beam.PCollection:\n",
        "\n",
        "  input_wet_mx_ic, error_read_wet_mx_ic = read_excels_union(pipeline, get_wet_mx_intracompany_path(), identifier='wet_mx_ic', skiprows=1, column_date=['Date'], converters={'Item':str})\n",
        "  error_read_wet_mx_ic  | 'Handle Format Errors Read WET MX Intracompany' >> beam.Map(log_error)\n",
        "\n",
        "  input_folder_wet_mx_ic, error_read_folder_wet_mx_ic = read_excels_union(pipeline, get_wet_folder_mx_intracompany_path(), identifier='folder_wet_mx_ic', skiprows=1, column_date=['Date'], converters={'Item':str})\n",
        "  error_read_folder_wet_mx_ic | 'Handle Format Errors Read Folder WET MX Intracompany' >> beam.Map(log_error)\n",
        "\n",
        "  merged_wet_mx_ic = ((input_wet_mx_ic,input_folder_wet_mx_ic) | 'Merge PCollections WET MX IC' >> beam.Flatten())\n",
        "\n",
        "  stage_wet_mx_ic, error_wet_mx_ic = (merged_wet_mx_ic\n",
        "      | 'Clean NaN wet mx ic' >> beam.ParDo(dc.CleanNaN())\n",
        "      | 'Drop Missing Value in Transaction Type' >> beam.ParDo(dc.FilterColumnValues(column='Transaction Type', values_to_filter=[None, '']))\n",
        "      | 'Convert Columns Order Number & Line to Integer WET MX' >> beam.ParDo(de.ColumnsToIntegerConverter(['Order Number','Line']))\n",
        "      | 'Derive PURCHASE_ORDER by MERGING WET MX IC' >> beam.ParDo(de.MergeColumnsFn([(['Order Number','Line'],'PURCHASE_ORDER', '-')]))\n",
        "      | 'Drop Columns WET MC IC' >> beam.ParDo(dc.DropColumns(['Receipt','Packing Slip','Bill of Lading','Order Number','Line','Cost','Transaction Type','Unnamed: 0']))\n",
        "      | 'Rename Columns WET MX IC' >> beam.ParDo(dc.RenameColumns(rename_columns))\n",
        "      | 'Trim WET MX IC' >> beam.ParDo(TrimValues(['MATERIAL', 'VENDOR_NAME_LOCAL', 'VENDOR', 'PURCHASE_ORDER']))\n",
        "      | 'Replace Values in SOURCE  WET MX IC' >> beam.ParDo(dc.ReplaceValues([('MATERIAL', ' ', ''), ('VENDOR', ' ', ''),  ('PURCHASE_ORDER', 'NONE', ''), ('SOURCE','gs://nidec-ga-transient/', '')]))\n",
        "      | 'Derive ORG wet mx' >> beam.ParDo(de.GenericDeriveCondition(column=None, map=None, new_column='ORG', default='A02'))\n",
        "      | 'Derive TRANSACTION_CURRENCY' >> beam.ParDo(de.GenericDeriveCondition(column=None, map=None, new_column='TRANSACTION_CURRENCY', default='USD'))\n",
        "      | 'Convert Columns to Float WET MX IC' >> beam.ParDo(de.ColumnsToFloatConverter(['SPEND_PO_TC','TRX_QTY']))\n",
        "      | 'Convert Columns to String WET MX IC' >> beam.ParDo(de.ColumnsToStringConverter(['ORG', 'PURCHASE_ORDER', 'MATERIAL', 'VENDOR', 'SOURCE', 'VENDOR_NAME_LOCAL','UOM','TRANSACTION_CURRENCY']))\n",
        "      #| 'Replace Patters Material startswith 0 for \"\"  WET MX IC' >> beam.ParDo(ReplacePatterns(columns=['MATERIAL'],pattern=r'^0+', replacement=''))\n",
        "      | 'Convert String Columns to UPPERCASE WET MX IC' >> beam.ParDo(de.ConvertToUpperCase(['ORG', 'PURCHASE_ORDER', 'MATERIAL', 'VENDOR', 'SOURCE', 'VENDOR_NAME_LOCAL','UOM','TRANSACTION_CURRENCY']))\n",
        "      | 'Derive SPEND_PO_LC from SPEND_PO_TC' >> beam.ParDo(de.ColumnCopy('SPEND_PO_TC','SPEND_PO_LC'))\n",
        "      | \"Ensure Schema Order wet mx in\" >> beam.ParDo(EnsureSchemaOrderDoFn(schema_pbi_nsap))\n",
        "      | 'Verify Column Date Format WET MX IC' >> beam.ParDo(VerifyStrictDateFormat(date_column='PERIOD')).with_outputs('error', main='main')\n",
        "  )\n",
        "  error_wet_mx_ic | 'Handle Format Errors WET MX Intracompany' >> beam.Map(log_error)\n",
        "\n",
        "  return stage_wet_mx_ic\n"
      ],
      "metadata": {
        "id": "bZ6DucW3P42O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_wet_cn() -> beam.PCollection:\n",
        "\n",
        "  input_wet_cn, error_read_wet_cn = read_excels_union(pipeline, get_wet_cn_path(), identifier='wet_cn', parse_dates=['RECV_DATE'], converters={'ITM_GLMOCO':str,'VENDOR_ID':str})\n",
        "  error_read_wet_cn | 'Handle Format Errors Read WET CN' >> beam.Map(log_error)\n",
        "\n",
        "  input_folder_wet_cn, error_read_folder_wet_cn = read_excels_union(pipeline, get_wet_folder_cn_path(), identifier='wet_folder_cn', parse_dates=['RECV_DATE'], converters={'ITM_GLMOCO':str,'VENDOR_ID':str})\n",
        "  error_read_folder_wet_cn | 'Handle Format Errors Read Folder WET CN' >> beam.Map(log_error)\n",
        "\n",
        "  merged_wet_cn = ((input_wet_cn,input_folder_wet_cn) | 'Merge PCollections WET CN' >> beam.Flatten())\n",
        "\n",
        "  stage_wet_cn, error_wet_cn = (merged_wet_cn\n",
        "      | 'Rename Columns WET CN' >> beam.ParDo(dc.RenameColumns(rename_columns))\n",
        "      | 'Derive SPEND_PO_TC with UNIT_COST * TRX_QTY' >> beam.ParDo(de.GenericArithmeticOperation([{'operands':['UNIT_COST','TRX_QTY'],'result_column':'SPEND_PO_TC','formula': lambda c1,c2: c1 * c2 }]))\n",
        "      | 'Derive SPEND_PO_TC with UNIT_COST / EXCH_RATE1' >> beam.ParDo(de.GenericArithmeticOperation([{'operands':['SPEND_PO_TC','EXCH_RATE1'],'result_column':'SPEND_PO_LC','formula': lambda c1,c2: c1 / c2 if c2 else 0 }]))\n",
        "      | 'Derive ORG wet cn' >> beam.ParDo(de.GenericDeriveCondition(column='ORG', map={'GA':'B01', 'COM':'B00'}, new_column='ORG', default='MISSING')) # para validar com a tabela será feito depois.\n",
        "      | 'Clean NaN to write in wet cn' >> beam.ParDo(dc.CleanNaN())\n",
        "      | 'Drop Missing Value in PERIOD' >> beam.ParDo(dc.FilterColumnValues(column='PERIOD', values_to_filter=[None, '']))\n",
        "      | 'Trim WET CN' >> beam.ParDo(TrimValues(['MATERIAL', 'VENDOR_NAME_LOCAL', 'VENDOR', 'PURCHASE_ORDER']))\n",
        "      | 'Replace Values in  WET CN' >> beam.ParDo(dc.ReplaceValues([('MATERIAL', ' ', ''), ('VENDOR', ' ', ''),  ('SOURCE', 'gs://nidec-ga-transient/', '')]))\n",
        "      | 'Convert Columns to Float WET CN' >> beam.ParDo(de.ColumnsToFloatConverter(['TRX_QTY','UNIT_COST','SPEND_PO_TC','SPEND_PO_LC','EXCH_RATE1']))\n",
        "      | 'Convert Columns to String WET CN' >> beam.ParDo(de.ColumnsToStringConverter(['PERIOD','ORG', 'PURCHASE_ORDER', 'MATERIAL', 'CATEGORY_LOCAL', 'DRI_CODE', 'VENDOR', 'VENDOR_NAME_LOCAL', 'TRANSACTION_CURRENCY', 'SOURCE']))\n",
        "      #| 'Replace Patters Material startswith 0 for \"\"  WET CN' >> beam.ParDo(ReplacePatterns(columns=['MATERIAL'],pattern=r'^0+', replacement=''))\n",
        "      | 'Convert String Columns to UPPERCASE WET CN' >> beam.ParDo(de.ConvertToUpperCase(['ORG', 'PURCHASE_ORDER', 'MATERIAL', 'CATEGORY_LOCAL', 'DRI_CODE', 'VENDOR', 'VENDOR_NAME_LOCAL', 'TRANSACTION_CURRENCY', 'SOURCE']))\n",
        "      | 'Keep Columns WET CN' >> beam.ParDo(dc.KeepColumns(['ORG', 'PERIOD', 'PURCHASE_ORDER', 'MATERIAL', 'VENDOR', 'TRX_QTY', 'SPEND_PO_TC', 'SPEND_PO_LC', 'TRANSACTION_CURRENCY', 'SOURCE', 'VENDOR_NAME_LOCAL', 'CATEGORY_LOCAL', 'DRI_CODE']))\n",
        "      | \"Ensure Schema Order WET CN\" >> beam.ParDo(EnsureSchemaOrderDoFn(schema_pbi_nsap))\n",
        "      | 'Drop Missing Value in PERIOD WET CN' >> beam.ParDo(dc.FilterColumnValues(column='PERIOD', values_to_filter=[None, '', ' ']))\n",
        "      | 'Verify Column Date Format WET CN' >> beam.ParDo(VerifyStrictDateFormat(date_column='PERIOD')).with_outputs('error', main='main')\n",
        "  )\n",
        "  error_wet_cn | 'Handle Format Errors WET CN' >> beam.Map(log_error)\n",
        "\n",
        "  return stage_wet_cn"
      ],
      "metadata": {
        "id": "leUaQoF6P49K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_wet_ro() -> beam.PCollection:\n",
        "\n",
        "  input_wet_ro, error_read_wet_ro = read_excels_union(pipeline, get_wet_ro_path(), identifier='wet_ro', skiprows=3, converters={'Item':str, 'Sup':str})\n",
        "  error_read_wet_ro | 'Handle Format Errors Read WET RO' >> beam.Map(log_error)\n",
        "\n",
        "  input_wet_ro_ic, error_read_wet_ro_ic = read_excels_union(pipeline, get_wet_ro_intracompany_path(), identifier='wet_ro_i', skiprows=3, converters={'Item':str, 'Sup':str})\n",
        "  error_read_wet_ro_ic | 'Handle Format Errors Read WET RO Intracompany' >> beam.Map(log_error)\n",
        "\n",
        "  input_folder_wet_ro, error_read_folder_wet_ro = read_excels_union(pipeline, get_wet_folder_ro_path(), identifier='wet_folder_ro', skiprows=3, converters={'Item':str, 'Sup':str})\n",
        "  error_read_folder_wet_ro | 'Handle Format Errors Read Folder WET RO' >> beam.Map(log_error)\n",
        "\n",
        "  merged_wet_ro = ((input_wet_ro, input_folder_wet_ro, input_wet_ro_ic) | 'Merge PCollections WET RO' >> beam.Flatten())\n",
        "\n",
        "  stage_wet_ro, error_wet_ro = (merged_wet_ro\n",
        "      | 'Clean NaN Wet RO' >> beam.ParDo(dc.CleanNaN())\n",
        "      | 'Drop Missing Value in INT_EXT' >> beam.ParDo(dc.FilterColumnValues(column='INT_EXT', values_to_filter=[None, '']))\n",
        "      | 'Filter Col INT_EXT with INT_EXT value' >> beam.ParDo(dc.FilterColumnValues('INT_EXT','INT_EXT'))\n",
        "      | 'Derive Test Cooling wet ro' >> beam.ParDo(de.ColumnValueAssignment(value='01', new_column='day'))\n",
        "      | 'Convert Columns to integer before convert to string WET RO' >> beam.ParDo(de.ColumnsToIntegerConverter(['Year','MTH']))\n",
        "      | 'Convert Columns to String WET RO DATE' >> beam.ParDo(de.ColumnsToStringConverter(['Year','MTH','day']))\n",
        "      | 'Derive Period wet ro' >> beam.ParDo(de.MergeColumnsFn([(['Year','MTH','day'],'PERIOD','-')]))\n",
        "      | 'Keep Columns WET RO' >> beam.ParDo(dc.KeepColumns(['ORG','PERIOD', 'Item', 'Sup', 'Company', 'Gross TO', 'Pu Qty', 'TransactionCurrency', 'INT_EXT', 'Cm_Descr', 'Item Descr','SOURCE', 'Supplier','TO local currency']))\n",
        "      | 'Rename Columns WET RO' >> beam.ParDo(dc.RenameColumns({'INT_EXT': 'VENDOR_TYPE', 'Cm_Descr': 'CATEGORY_LOCAL', 'Item': 'MATERIAL', 'Item Descr': 'MATERIAL_DESCRIPTION', 'Sup': 'VENDOR', 'Supplier': 'VENDOR_NAME_LOCAL', 'Company': 'ORG', 'Gross TO': 'SPEND_PO_LC', 'TO local currency': 'SPEND_PO_LC' , 'Pu Qty': 'TRX_QTY', 'TransactionCurrency': 'TRANSACTION_CURRENCY'}))\n",
        "      | 'Trim WET RO' >> beam.ParDo(TrimValues(['MATERIAL','MATERIAL_DESCRIPTION', 'VENDOR']))\n",
        "      | 'Replace Values in SOURCE  WET RO' >> beam.ParDo(dc.ReplaceValues([('MATERIAL', ' ', ''), ('VENDOR', ' ', ''), ('SPEND_PO_LC', ',', ''), ('TRX_QTY', ',', ''), ('SOURCE', 'gs://nidec-ga-transient/', '')]))\n",
        "      | 'Convert Columns to Float WET RO' >> beam.ParDo(de.ColumnsToFloatConverter(['TRX_QTY', 'SPEND_PO_LC']))\n",
        "      | 'Multiply TRX and SPEND by 1000' >> beam.ParDo(de.MultiplyColumns(columns=['TRX_QTY', 'SPEND_PO_LC'],factor=1000.0))\n",
        "      | 'Convert Columns to String WET RO' >> beam.ParDo(de.ColumnsToStringConverter(['ORG', 'MATERIAL', 'VENDOR', 'TRANSACTION_CURRENCY', 'SOURCE', 'VENDOR_TYPE', 'CATEGORY_LOCAL', 'MATERIAL_DESCRIPTION', 'VENDOR_NAME_LOCAL']))\n",
        "      | 'Convert String Columns to UPPERCASE WET RO' >> beam.ParDo(de.ConvertToUpperCase(['ORG', 'MATERIAL', 'VENDOR', 'TRANSACTION_CURRENCY', 'SOURCE', 'VENDOR_TYPE', 'CATEGORY_LOCAL', 'MATERIAL_DESCRIPTION', 'VENDOR_NAME_LOCAL']))\n",
        "      #| 'Replace Patters Material startswith 0 for \"\"  WET RO' >> beam.ParDo(ReplacePatterns(columns=['MATERIAL'],pattern=r'^0+', replacement=''))\n",
        "      | \"Ensure Schema Order WET RO\" >> beam.ParDo(EnsureSchemaOrderDoFn(schema_pbi_nsap))\n",
        "      | 'Drop Missing Value in PERIOD WET RO' >> beam.ParDo(dc.FilterColumnValues(column='PERIOD', values_to_filter=[None, '', ' ']))\n",
        "      | 'Verify Column Date Format WET RO' >> beam.ParDo(VerifyStrictDateFormat(date_column='PERIOD')).with_outputs('error', main='main')\n",
        "  )\n",
        "  error_wet_ro | 'Handle Format Errors WET RO' >> beam.Map(log_error)\n",
        "\n",
        "\n",
        "  return stage_wet_ro"
      ],
      "metadata": {
        "id": "7bzkR94TP5Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_hvac_cn() -> beam.PCollection:\n",
        "\n",
        "  input_hvac_cn, error_read_hvac_cn = read_excels_union(pipeline, get_hvac_cn_path(), identifier='hvac_cn', converters={'Item Number':str, 'Vendor ID':str})\n",
        "  input_hvac_cn_nm = (input_hvac_cn | 'Rename Column Order Qty to match in all input_hvac_cn' >> beam.ParDo(dc.RenameColumns({'Order Qty': 'Quantity'})))\n",
        "  error_read_hvac_cn | 'Handle Format Errors Read HVAC CN' >> beam.Map(log_error)\n",
        "\n",
        "  input_folder_hvac_cn, error_read_folder_hvac_cn = read_excels_union(pipeline, get_hvac_folder_cn_path(), identifier='hvac_folder_cn', converters={'Item Number':str, 'Vendor ID':str})\n",
        "  input_folder_hvac_cn_rnm = (input_folder_hvac_cn | 'Rename Column Order Qty to match in all input_folder_hvac_cn' >> beam.ParDo(dc.RenameColumns({'Order Qty': 'Quantity'})))\n",
        "  error_read_folder_hvac_cn | 'Handle Format Errors Read Folder HVAC CN' >> beam.Map(log_error)\n",
        "\n",
        "  merged_hvac_cn = (( input_hvac_cn_nm,input_folder_hvac_cn_rnm ) | 'Merge PCollections HVAC CN' >> beam.Flatten())\n",
        "\n",
        "  stage_hvac_cn, error_hvac_cn = ( merged_hvac_cn\n",
        "      | 'Clean NaN HVAC CN' >> beam.ParDo(dc.CleanNaN())\n",
        "      | 'Drop Missing Value in Year HVAC CN' >> beam.ParDo(dc.FilterColumnValues(column='Year', values_to_filter=[None, '']))\n",
        "      | 'Derive Day HVAC CN' >> beam.ParDo(de.ColumnValueAssignment(value='01', new_column='day'))\n",
        "      | 'Derive ORG HVAC CN' >> beam.ParDo(de.GenericDeriveCondition(column=None, map=None, new_column='ORG', default='B02'))\n",
        "      | 'Convert Columns to integer before convert to string HVAC CN' >> beam.ParDo(de.ColumnsToIntegerConverter(['Year','Period','day']))\n",
        "      | 'Convert Columns to String HVAC CN date' >> beam.ParDo(de.ColumnsToStringConverter(['Year','Period','day']))\n",
        "      | 'Derive Period' >> beam.ParDo(de.MergeColumnsFn([(['Year','Period','day'],'PERIOD','-')]))\n",
        "      | 'Keep Columns HVAC CN' >> beam.ParDo(dc.KeepColumns(['ORG','PONumber', 'Vendor ID', 'Item Number', 'PERIOD', 'Quantity', 'Foreign Invoice Amount', 'Currency Code', 'POReceipt Local Amount', 'Vendor EN_Name', 'Item Description', 'Item Description.1', 'SOURCE']))\n",
        "      | 'Rename Columns HVAC CN' >> beam.ParDo(dc.RenameColumns(rename_columns))\n",
        "      | 'Trim HVAC CN' >> beam.ParDo(TrimValues(['MATERIAL','INCOTERM', 'PAYMENT_TERM','VENDOR', 'PURCHASE_ORDER']))\n",
        "      | 'Replace Values in SOURCE  HVAC CN' >> beam.ParDo(dc.ReplaceValues([('MATERIAL', ' ', ''),  ('VENDOR', ' ', ''), ('SOURCE','gs://nidec-ga-transient/','')]))\n",
        "      | 'Convert Columns to Float HVAC CN' >> beam.ParDo(de.ColumnsToFloatConverter(['TRX_QTY','SPEND_PO_LC','SPEND_PO_TC']))\n",
        "      | 'Convert Columns to String HVAC CN' >> beam.ParDo(de.ColumnsToStringConverter(['ORG', 'MATERIAL', 'VENDOR', 'TRANSACTION_CURRENCY', 'SOURCE', 'CATEGORY_LOCAL', 'MATERIAL_DESCRIPTION', 'VENDOR_NAME_LOCAL']))\n",
        "      | 'Convert String Columns to UPPERCASE HVAC CN' >> beam.ParDo(de.ConvertToUpperCase(['ORG', 'MATERIAL', 'VENDOR', 'TRANSACTION_CURRENCY', 'SOURCE', 'CATEGORY_LOCAL', 'MATERIAL_DESCRIPTION', 'VENDOR_NAME_LOCAL']))\n",
        "      #| 'Replace Patters Material startswith 0 for \"\"  HVAC CN' >> beam.ParDo(ReplacePatterns(columns=['MATERIAL'],pattern=r'^0+', replacement=''))\n",
        "      | \"Ensure Schema Order HVAC CN\" >> beam.ParDo(EnsureSchemaOrderDoFn(schema_pbi_nsap))\n",
        "      | 'Drop Missing Value in PERIOD HVAC CN' >> beam.ParDo(dc.FilterColumnValues(column='PERIOD', values_to_filter=[None, '', ' ']))\n",
        "      | 'Verify Column Date Format HVAC CN' >> beam.ParDo(VerifyStrictDateFormat(date_column='PERIOD')).with_outputs('error', main='main')\n",
        "  )\n",
        "  error_hvac_cn | 'Handle Format Errors HVAC CN' >> beam.Map(log_error)\n",
        "\n",
        "  return stage_hvac_cn"
      ],
      "metadata": {
        "id": "26gbXTBpP5Kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cimd_emea() -> beam.PCollection:\n",
        "\n",
        "    input_cimd_emea_dados_transacionais, error_read_cimd_emea_dt = read_excels_union(pipeline, get_cimd_emea_path(), identifier='cimd_emea dados transacionais', skiprows=2, tab='Receipt Book',column_date=['Receipt Date'], converters={'Part Number':str, 'Supplier ID':str})\n",
        "    error_read_cimd_emea_dt | 'Handle Format Errors Read CIMD EMEA DADOS TRANSACIONAIS' >> beam.Map(log_error)\n",
        "\n",
        "    input_folder_cimd_emea_dados_transacionais, error_read_folder_cimd_emea_dt = read_excels_union(pipeline, get_cimd_folder_emea_path(), identifier='folder_cimd_emea dados transacionais', skiprows=2, tab='Receipt Book',column_date=['Receipt Date'], converters={'Part Number':str, 'Supplier ID':str})\n",
        "    error_read_folder_cimd_emea_dt | 'Handle Format Errors Read Folder CIMD EMEA DADOS TRANSACIONAIS' >> beam.Map(log_error)\n",
        "\n",
        "    merged_cimd_emea_dados_transacionais = ((input_cimd_emea_dados_transacionais,input_folder_cimd_emea_dados_transacionais) | 'Merge PCollections CIMD EMEA dados transacionais' >> beam.Flatten())\n",
        "\n",
        "    clean_cimd_emea_dados_transacionais, error_cimd_emea_dt = (merged_cimd_emea_dados_transacionais\n",
        "        | 'Drop Columns CIMD EMEA dados transacionais' >> beam.ParDo(dc.DropColumns(['Unnamed: 12', 'Unnamed: 13']))\n",
        "        | 'Rename Columns CIMD EMEA dados transacionais' >> beam.ParDo(dc.RenameColumns({'OU Code (Legal entity)': 'ORG', 'Receipt Date': 'PERIOD', 'Supplier ID': 'VENDOR', 'PO Number': 'PURCHASE_ORDER', 'Part Number': 'MATERIAL', 'Receipt Quantity (LS UOM)': 'TRX_QTY', 'Receipt Amount (PO currency)': 'SPEND_PO_TC', 'PO Currency': 'TRANSACTION_CURRENCY', 'Receipt Amount (Local Currency)': 'SPEND_PO_LC', 'Incoterm Code': 'INCOTERM', 'Payment Terms (SUPP)': 'PAYMENT_TERM', 'Local Currency': 'LOCAL_CURRENCY'}))\n",
        "        | 'Filter Col ORG with ORG and LS08 value' >> beam.ParDo(dc.FilterColumnValues(column='ORG', values_to_filter=['LS08','ORG']))\n",
        "        | 'Convert Columns to String CIMD EMEA dados transacionais' >> beam.ParDo(de.ColumnsToStringConverter(['PERIOD', 'ORG', 'PURCHASE_ORDER', 'MATERIAL', 'VENDOR', 'TRANSACTION_CURRENCY', 'LOCAL_CURRENCY', 'PAYMENT_TERM', 'INCOTERM', 'SOURCE']))\n",
        "        #| 'Replace Patters Material startswith 0 for \"\"  CIMD EMEA' >> beam.ParDo(ReplacePatterns(columns=['MATERIAL'],pattern=r'^0+', replacement=''))\n",
        "        | 'Trim CIMD EMEA MD DT' >> beam.ParDo(TrimValues(['MATERIAL','INCOTERM', 'PAYMENT_TERM','VENDOR', 'PURCHASE_ORDER']))\n",
        "        | 'Replace Values Spaces in MATERIAL CIMD EMEA dados transacionais' >> beam.ParDo(dc.ReplaceValues([('MATERIAL', ' ', ''),  ('SPEND_PO_LC', ',', '.'), ('SOURCE', 'gs://nidec-ga-transient/', '')]))\n",
        "        | 'Convert Columns to Float CIMD EMEA dados transacionais' >> beam.ParDo(de.ColumnsToFloatConverter(['TRX_QTY', 'SPEND_PO_LC', 'SPEND_PO_TC']))\n",
        "        | 'Convert String Columns to UPPERCASE CIMD EMEA dados transacionais' >> beam.ParDo(de.ConvertToUpperCase(['ORG', 'PURCHASE_ORDER', 'MATERIAL', 'VENDOR', 'TRANSACTION_CURRENCY', 'LOCAL_CURRENCY', 'PAYMENT_TERM', 'INCOTERM', 'SOURCE']))\n",
        "        | 'Clean NaN dados transacionais' >> beam.ParDo(dc.CleanNaN())\n",
        "        | 'Drop Missing Value in MATERIAL MD dados transacionais' >> beam.ParDo(dc.FilterColumnValues(column='MATERIAL', values_to_filter=[None, '']))\n",
        "        | 'Drop Missing Value in PERIOD CIMD EMEA' >> beam.ParDo(dc.FilterColumnValues(column='PERIOD', values_to_filter=[None, '', ' ']))\n",
        "        | 'Drop columns CIMD EMEA dados transionais out left join' >> beam.ParDo(dc.DropColumns(['VENDOR_NAME_LOCAL', 'VENDOR_COUNTRY','VENDOR_TYPE','MATERIAL_DESCRIPTION','UOM','CATEGORY_LOCAL','FAMILY_LOCAL','DRI_CODE']))\n",
        "        | 'Verify Column Date Format CIMD EMEA' >> beam.ParDo(VerifyStrictDateFormat(date_column='PERIOD')).with_outputs('error', main='main')\n",
        "    )\n",
        "    error_cimd_emea_dt | 'Handle Format Errors CIMD EMEA DADOS TRANSACIONAIS' >> beam.Map(log_error)\n",
        "\n",
        "    cimd_emea_md_material = build_cimd_emea_md_material()\n",
        "\n",
        "    group_clean_cimd_emea_dados_transacionais = de.key_transform(clean_cimd_emea_dados_transacionais, ['MATERIAL'], identifier='dados transacionais para materiais')\n",
        "    group_clean_cimd_emea_md_material = de.key_transform(cimd_emea_md_material, ['MATERIAL'], identifier='master data materiais')\n",
        "\n",
        "    cimd_dados_transacionais_material = de.join(group_clean_cimd_emea_dados_transacionais, group_clean_cimd_emea_md_material, method='left_join', columns_to_include=['MATERIAL_DESCRIPTION','UOM', 'CATEGORY_LOCAL','FAMILY_LOCAL','DRI_CODE'])\n",
        "\n",
        "    cimd_emea_md_vendor = build_cimd_emea_md_vendor()\n",
        "    group_cimd_dados_transacionais_material = de.key_transform(cimd_dados_transacionais_material, ['VENDOR'], identifier='dados transacionais para vendor')\n",
        "    group_clean_cimd_emea_md_vendor = de.key_transform(cimd_emea_md_vendor, ['VENDOR'], identifier='dados transacionais')\n",
        "\n",
        "    stage_cimd_emea = de.join(group_cimd_dados_transacionais_material, group_clean_cimd_emea_md_vendor, method='left_join', columns_to_include=['VENDOR_NAME_LOCAL', 'VENDOR_COUNTRY', 'VENDOR_TYPE']) | \"Ensure Schema Order CIMD EMEA\" >> beam.ParDo(EnsureSchemaOrderDoFn(schema_pbi_nsap))\n",
        "\n",
        "    return stage_cimd_emea"
      ],
      "metadata": {
        "id": "frpYkTqDP5Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cimd_emea_md_material() -> beam.PCollection:\n",
        "\n",
        "    input_cimd_emea_md_material, error_read_cimd_emea_material = read_excels_union(pipeline, get_cimd_emea_path(), identifier='cimd_emea', skiprows=2, tab='Material', converters={'Part Number':str})\n",
        "    error_read_cimd_emea_material | 'Handle Format Errors Read CIMD EMEA DADOS MATERIAL' >> beam.Map(log_error)\n",
        "\n",
        "    input_folder_cimd_emea_md_material, error_read_folder_cimd_emea_material = read_excels_union(pipeline, get_cimd_folder_emea_path(), identifier='folder_cimd_emea', skiprows=2, tab='Material', converters={'Part Number':str})\n",
        "    error_read_folder_cimd_emea_material | 'Handle Format Errors Read Folder CIMD EMEA DADOS MATERIAL' >> beam.Map(log_error)\n",
        "\n",
        "    merged_cimd_emea_md_material = ((input_cimd_emea_md_material,input_folder_cimd_emea_md_material) | 'Merge PCollections CIMD EMEA' >> beam.Flatten())\n",
        "\n",
        "    clean_cimd_emea_md_material = (merged_cimd_emea_md_material\n",
        "        | 'Rename Columns CIMD EMEA MD MATERIAL' >> beam.ParDo(dc.RenameColumns({'OU Code (Legal entity)': 'ORG',\n",
        "                                                                                    'Part Number': 'MATERIAL', 'Part Long Description': 'MATERIAL_DESCRIPTION','Primary UOM (LS)': 'UOM', 'Parent ICC Name (ICC)': 'CATEGORY_LOCAL', 'ICC Name': 'FAMILY_LOCAL',\n",
        "                                                                                    'DRI': 'DRI_CODE'}))\n",
        "        | 'Drop Columns CIMD EMEA MD MATERIAL' >> beam.ParDo(dc.DropColumns(['Unnamed: 6','ORG']))\n",
        "        | 'Trim CIMD EMEA MD MATERIAL' >> beam.ParDo(TrimValues(['MATERIAL','MATERIAL_DESCRIPTION', 'VENDOR', 'UOM','CATEGORY_LOCAL','FAMILY_LOCAL','DRI_CODE']))\n",
        "        | 'Replace Values in SOURCE  CIMD EMEA MD MATERIAL' >> beam.ParDo(dc.ReplaceValues([('MATERIAL', ' ', ''), ('VENDOR', ' ', ''), ('MATERIAL_DESCRIPTION', '.,', '. '), ('SOURCE','gs://nidec-ga-transient/','')]))\n",
        "        | 'Convert Columns to String CIMD EMEA MD MATERIAL' >> beam.ParDo(de.ColumnsToStringConverter(['MATERIAL', 'MATERIAL_DESCRIPTION', 'UOM', 'CATEGORY_LOCAL', 'FAMILY_LOCAL', 'DRI_CODE','ORG', 'SOURCE']))\n",
        "        | 'Convert String Columns to UPPERCASE CIMD EMEA  MD MATERIAL' >> beam.ParDo(de.ConvertToUpperCase(['MATERIAL', 'MATERIAL_DESCRIPTION', 'UOM', 'CATEGORY_LOCAL', 'FAMILY_LOCAL', 'DRI_CODE','ORG', 'SOURCE']))\n",
        "        #| 'Replace Patters Material startswith 0 for \"\"  CIMD EMEA MATERIAL' >> beam.ParDo(ReplacePatterns(columns=['MATERIAL'],pattern=r'^0+', replacement=''))\n",
        "        | 'Drop Missing Value in MATERIAL MD MATERIAL' >> beam.ParDo(dc.FilterColumnValues(column='MATERIAL', values_to_filter=[None, '']))\n",
        "        | \"Extract Date in YYYYMMDD material\" >> beam.ParDo(ExtractDateYYYYMMDD(column_name=\"SOURCE\"))\n",
        "        | 'Keep columns to left in Material' >> beam.ParDo(dc.KeepColumns(['MATERIAL','MATERIAL_DESCRIPTION','UOM','CATEGORY_LOCAL','FAMILY_LOCAL','DRI_CODE','EXTRACT_DATE','SOURCE']))\n",
        "        | 'deduplicate material' >> beam.ParDo(DeduplicateByColumnFn(['MATERIAL', 'EXTRACT_DATE']))\n",
        "    )\n",
        "\n",
        "    parse_and_group_clean_cimd_emea_md_material = (clean_cimd_emea_md_material\n",
        "        | 'Parse Date and Key Material' >> beam.ParDo(MaxDate(keys=['MATERIAL'], date_column='EXTRACT_DATE'))\n",
        "        | 'Group by Material' >> beam.GroupByKey()\n",
        "    )\n",
        "\n",
        "    selected_rows_clean_cimd_emea_md_material = (parse_and_group_clean_cimd_emea_md_material\n",
        "        | 'Select Max Date Row Material'  >> beam.ParDo(FilterMaxDate())\n",
        "    )\n",
        "\n",
        "    return selected_rows_clean_cimd_emea_md_material"
      ],
      "metadata": {
        "id": "Nw3vsE2u49NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cimd_emea_md_vendor() -> beam.PCollection:\n",
        "\n",
        "    input_cimd_emea_md_vendor, error_read_cimd_emea_vendor = read_excels_union(pipeline, get_cimd_emea_path(), identifier='cimd_emea_vendor', skiprows=2, tab='Vendor', converters={'Supplier ID':str})\n",
        "    error_read_cimd_emea_vendor | 'Handle Format Errors Read CIMD EMEA VENDOR' >> beam.Map(log_error)\n",
        "\n",
        "    input_folder_cimd_emea_md_vendor, error_read_folder_cimd_emea_vendor = read_excels_union(pipeline, get_cimd_folder_emea_path(), identifier='folder_cimd_emea_vendor', skiprows=2, tab='Vendor', converters={'Supplier ID':str})\n",
        "    error_read_folder_cimd_emea_vendor | 'Handle Format Errors Read Folder CIMD EMEA VENDOR' >> beam.Map(log_error)\n",
        "\n",
        "    merged_cimd_emea_md_vendor = ((input_cimd_emea_md_vendor,input_folder_cimd_emea_md_vendor) | 'Merge PCollections CIMD EMEA _vendor' >> beam.Flatten())\n",
        "\n",
        "    clean_cimd_emea_md_vendor = (merged_cimd_emea_md_vendor\n",
        "        | 'Rename Columns CIMD EMEA MD vendor' >> beam.ParDo(dc.RenameColumns({'OU Code (Legal entity)':'ORG','ORG CODE': 'ORG','Supplier ID': 'VENDOR','Supplier Name': 'VENDOR_NAME_LOCAL','Country Code (SUPP)': 'VENDOR_COUNTRY','Supplier Category ': 'VENDOR_TYPE'}))\n",
        "        | 'Drop Columns CIMD EMEA MD _vendor' >> beam.ParDo(dc.DropColumns(['Unnamed: 2','Unnamed: 5','  ','Unnamed: 6','ORG']))\n",
        "        | 'Trim CIMD EMEA VENDOR' >> beam.ParDo(TrimValues(['VENDOR', 'MATERIAL']))\n",
        "        | 'Replace Values Spaces in MATERIAL CIMD EMEA MD _vendor' >> beam.ParDo(dc.ReplaceValues([('MATERIAL', ' ', ''), ('VENDOR', ' ', ''), ('VENDOR_NAME_LOCAL', '.,', '. ')]))\n",
        "        | 'Replace Values in SOURCE  CIMD EMEA MD _vendor' >> beam.ParDo(dc.ReplaceValues([('SOURCE','gs://nidec-ga-transient/','')]))\n",
        "        | 'Convert Columns to String CIMD EMEA MD _vendor' >> beam.ParDo(de.ColumnsToStringConverter(['ORG', 'VENDOR', 'VENDOR_NAME_LOCAL', 'VENDOR_COUNTRY', 'VENDOR_TYPE', 'SOURCE']))\n",
        "        #| 'Replace Patters Material startswith 0 for \"\"  WET RO VENDOR' >> beam.ParDo(ReplacePatterns(columns=['MATERIAL'],pattern=r'^0+', replacement=''))\n",
        "        | 'Convert String Columns to UPPERCASE CIMD EMEA  MD _vendor' >> beam.ParDo(de.ConvertToUpperCase(['ORG', 'VENDOR', 'VENDOR_NAME_LOCAL', 'VENDOR_COUNTRY', 'VENDOR_TYPE', 'SOURCE']))\n",
        "        | 'Clean NaN MD VENDOR' >> beam.ParDo(dc.CleanNaN())\n",
        "        | 'Drop Missing Value in VENDOR MD VENDOR' >> beam.ParDo(dc.FilterColumnValues(column='VENDOR', values_to_filter=[None, '']))\n",
        "        | \"Extract Date in YYYYMMDD VENDOR\" >> beam.ParDo(ExtractDateYYYYMMDD(column_name=\"SOURCE\"))\n",
        "        | 'Keep Columns to left in vendor' >> beam.ParDo(dc.KeepColumns(['VENDOR','VENDOR_NAME_LOCAL', 'VENDOR_COUNTRY','VENDOR_TYPE','EXTRACT_DATE']))\n",
        "        | 'deduplicate VENDOR' >> beam.ParDo(DeduplicateByColumnFn(['VENDOR', 'EXTRACT_DATE']))\n",
        "    )\n",
        "\n",
        "    parse_and_group_clean_cimd_emea_md_vendor = (clean_cimd_emea_md_vendor\n",
        "        | 'Parse Date and Key Vendor' >> beam.ParDo(MaxDate(keys=['VENDOR'], date_column='EXTRACT_DATE'))\n",
        "        | 'Group by Vendor' >> beam.GroupByKey()\n",
        "    )\n",
        "\n",
        "    selected_rows_clean_cimd_emea_md_vendor = (parse_and_group_clean_cimd_emea_md_vendor\n",
        "        | 'Select Max Date Row Vendor' >> beam.ParDo(FilterMaxDate())\n",
        "    )\n",
        "\n",
        "    return selected_rows_clean_cimd_emea_md_vendor"
      ],
      "metadata": {
        "id": "J40-whWx49e5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KP1kt-bXna1c"
      },
      "outputs": [],
      "source": [
        "def build_pbi_pfx_nsap(temp_location, output_pbi_pfx_nsap):\n",
        "\n",
        "  pbi_pfx_nsap, error_missing_values = ((\n",
        "                                          build_wet_mx_intracompany(),\n",
        "                                          build_wet_ro(),\n",
        "                                          build_wet_cn(),\n",
        "                                          build_hvac_cn(),\n",
        "                                          build_cimd_emea(),\n",
        "                                          build_fir(),\n",
        "                                          build_cimd_in(),\n",
        "                                          build_oracle_nar()\n",
        "                                        )\n",
        "      | 'Union PCollections for pbi nsap' >> beam.Flatten()\n",
        "      | 'Clean' >> beam.ParDo(dc.CleanNaN())\n",
        "      | 'Drop MANUTORD MATERIAL' >> beam.ParDo(dc.FilterColumnValues(column='MATERIAL', values_to_filter=['MANUTORD']))\n",
        "      | 'Drop rows with CATEGORY_LOCAL == oth MISCELLANEOUS' >> beam.ParDo(dc.FilterColumnValues('CATEGORY_LOCAL',['OTH MISCELLANEOUS LS']))\n",
        "      | 'Verify Missing Values' >> beam.ParDo(CheckMissingValuesDoFn(column_list=['MATERIAL','VENDOR','PERIOD','TRX_QTY','SPEND_PO_LC'])).with_outputs('error', main='main')\n",
        "  )\n",
        "  error_missing_values | 'Got some missing values in final table' >> beam.Map(log_error)\n",
        "\n",
        "  pbi_pfx_nsap | 'Write To BigQuery -> PBI PFX NSAP' >> beam.io.WriteToBigQuery(\n",
        "      table=output_pbi_pfx_nsap,\n",
        "      schema=schema_pbi_nsap,\n",
        "      create_disposition = beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
        "      write_disposition = beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
        "      custom_gcs_temp_location = temp_location\n",
        "  )\n",
        "\n",
        "\n",
        "  pipeline.run().wait_until_finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  temp_location = make_path(\"nidec-ga-temp\", \"data-flow-pipelines\", \"pbi-footprint\", \"temp\")\n",
        "\n",
        "  output_pbi_pfx_nsap = 'nidec-ga:bq_trusted.PBI_PFX_NSAP_DF'\n",
        "\n",
        "  build_pbi_pfx_nsap(temp_location, output_pbi_pfx_nsap)"
      ],
      "metadata": {
        "id": "yx-LPxQYxUdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "483fe5cb-1be5-43dc-8bde-86bfd55ae1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    div.alert {\n",
              "      white-space: pre-line;\n",
              "    }\n",
              "  </style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
              "            <div class=\"alert alert-info\">No cache_root detected. Defaulting to staging_location gs://nidec-ga-temp/data-flow-pipelines/pbi-footprint/staging for cache location.</div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pk2A38PfY8ZP"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}